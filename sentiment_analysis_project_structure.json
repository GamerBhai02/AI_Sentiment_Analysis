{
  "data_collection": {
    "twitter_scraper.py": "\n# Twitter Data Collection with snscrape\nimport snscrape.modules.twitter as sntwitter\nimport pandas as pd\nfrom datetime import datetime\n\ndef scrape_tweets(keyword, limit=5000):\n    '''\n    Scrape tweets containing the specified keyword\n    '''\n    tweets_data = []\n\n    # Create search query\n    search_query = f\"{keyword} lang:en\"\n\n    # Scrape tweets\n    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(search_query).get_items()):\n        if i >= limit:\n            break\n\n        tweet_data = {\n            'id': tweet.id,\n            'date': tweet.date,\n            'content': tweet.content,\n            'username': tweet.username,\n            'reply_count': tweet.replyCount,\n            'retweet_count': tweet.retweetCount,\n            'like_count': tweet.likeCount,\n            'hashtags': [tag for tag in tweet.hashtags] if tweet.hashtags else [],\n            'url': tweet.url\n        }\n        tweets_data.append(tweet_data)\n\n    return pd.DataFrame(tweets_data)\n\n# Example usage\nif __name__ == \"__main__\":\n    keyword = \"artificial intelligence\"\n    df = scrape_tweets(keyword, limit=1000)\n    df.to_csv(f'tweets_{keyword.replace(\" \", \"_\")}.csv', index=False)\n    print(f\"Collected {len(df)} tweets about '{keyword}'\")\n        ",
    "reddit_scraper.py": "\n# Reddit Data Collection with PRAW\nimport praw\nimport pandas as pd\nfrom datetime import datetime\n\nclass RedditScraper:\n    def __init__(self, client_id, client_secret, user_agent):\n        self.reddit = praw.Reddit(\n            client_id=client_id,\n            client_secret=client_secret,\n            user_agent=user_agent\n        )\n\n    def scrape_posts(self, keyword, subreddits=['all'], limit=1000):\n        '''\n        Scrape Reddit posts containing the specified keyword\n        '''\n        posts_data = []\n\n        for subreddit_name in subreddits:\n            subreddit = self.reddit.subreddit(subreddit_name)\n\n            # Search for posts containing keyword\n            for submission in subreddit.search(keyword, limit=limit):\n                post_data = {\n                    'id': submission.id,\n                    'title': submission.title,\n                    'selftext': submission.selftext,\n                    'author': str(submission.author) if submission.author else '[deleted]',\n                    'score': submission.score,\n                    'upvote_ratio': submission.upvote_ratio,\n                    'num_comments': submission.num_comments,\n                    'created_utc': datetime.fromtimestamp(submission.created_utc),\n                    'subreddit': submission.subreddit.display_name,\n                    'url': submission.url,\n                    'permalink': submission.permalink\n                }\n\n                # Get top-level comments\n                submission.comments.replace_more(limit=0)\n                comments = []\n                for comment in submission.comments[:20]:  # Top 20 comments\n                    if hasattr(comment, 'body'):\n                        comments.append({\n                            'body': comment.body,\n                            'score': comment.score,\n                            'created_utc': datetime.fromtimestamp(comment.created_utc)\n                        })\n\n                post_data['comments'] = comments\n                posts_data.append(post_data)\n\n        return pd.DataFrame(posts_data)\n\n# Example usage\nif __name__ == \"__main__\":\n    scraper = RedditScraper(\n        client_id='YOUR_CLIENT_ID',\n        client_secret='YOUR_CLIENT_SECRET',\n        user_agent='sentiment_analyzer_v1.0'\n    )\n\n    keyword = \"artificial intelligence\"\n    df = scraper.scrape_posts(keyword, subreddits=['MachineLearning', 'artificial', 'technology'])\n    df.to_csv(f'reddit_posts_{keyword.replace(\" \", \"_\")}.csv', index=False)\n    print(f\"Collected {len(df)} posts about '{keyword}'\")\n        "
  },
  "preprocessing": {
    "text_preprocessor.py": "\n# Text Preprocessing Module\nimport re\nimport string\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nclass TextPreprocessor:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('english'))\n        self.lemmatizer = WordNetLemmatizer()\n\n    def clean_text(self, text):\n        '''\n        Clean and preprocess text data\n        '''\n        if pd.isna(text) or text == '':\n            return ''\n\n        # Convert to lowercase\n        text = text.lower()\n\n        # Remove URLs\n        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n\n        # Remove mentions (@username)\n        text = re.sub(r'@\\w+', '', text)\n\n        # Remove hashtags (#hashtag) - keep the text, remove the #\n        text = re.sub(r'#(\\w+)', r'\\1', text)\n\n        # Remove extra whitespaces\n        text = re.sub(r'\\s+', ' ', text)\n\n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n\n        # Tokenization\n        tokens = word_tokenize(text)\n\n        # Remove stopwords and lemmatize\n        tokens = [self.lemmatizer.lemmatize(word) for word in tokens \n                 if word not in self.stop_words and len(word) > 2]\n\n        return ' '.join(tokens)\n\n    def preprocess_dataset(self, df, text_column):\n        '''\n        Preprocess entire dataset\n        '''\n        df['cleaned_text'] = df[text_column].apply(self.clean_text)\n\n        # Filter out empty texts\n        df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n\n        return df\n\n# Example usage\nif __name__ == \"__main__\":\n    preprocessor = TextPreprocessor()\n\n    # Example text\n    sample_text = \"Check out this amazing #AI tool! https://example.com @username It's revolutionary! \ud83d\ude80\"\n    cleaned = preprocessor.clean_text(sample_text)\n    print(f\"Original: {sample_text}\")\n    print(f\"Cleaned: {cleaned}\")\n        "
  },
  "sentiment_analysis": {
    "distilbert_analyzer.py": "\n# DistilBERT Sentiment Analysis Module\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import pipeline\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass DistilBERTSentimentAnalyzer:\n    def __init__(self, model_name='distilbert-base-uncased-finetuned-sst-2-english'):\n        '''\n        Initialize DistilBERT sentiment analyzer\n        '''\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n        # Create sentiment pipeline\n        self.sentiment_pipeline = pipeline(\n            \"sentiment-analysis\", \n            model=self.model, \n            tokenizer=self.tokenizer,\n            device=0 if torch.cuda.is_available() else -1\n        )\n\n    def analyze_single_text(self, text):\n        '''\n        Analyze sentiment of a single text\n        '''\n        if not text or pd.isna(text):\n            return {'label': 'NEUTRAL', 'score': 0.0}\n\n        try:\n            result = self.sentiment_pipeline(text)[0]\n            return {\n                'label': result['label'],\n                'score': result['score']\n            }\n        except Exception as e:\n            print(f\"Error analyzing text: {e}\")\n            return {'label': 'NEUTRAL', 'score': 0.0}\n\n    def analyze_batch(self, texts, batch_size=32):\n        '''\n        Analyze sentiment for a batch of texts\n        '''\n        results = []\n\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing sentiment\"):\n            batch = texts[i:i+batch_size]\n            batch_results = []\n\n            for text in batch:\n                result = self.analyze_single_text(text)\n                batch_results.append(result)\n\n            results.extend(batch_results)\n\n        return results\n\n    def analyze_dataset(self, df, text_column):\n        '''\n        Analyze sentiment for entire dataset\n        '''\n        texts = df[text_column].tolist()\n        results = self.analyze_batch(texts)\n\n        # Extract labels and scores\n        df['sentiment_label'] = [result['label'] for result in results]\n        df['sentiment_score'] = [result['score'] for result in results]\n\n        # Map POSITIVE/NEGATIVE to more intuitive labels\n        label_mapping = {\n            'POSITIVE': 'Positive',\n            'NEGATIVE': 'Negative',\n            'NEUTRAL': 'Neutral'\n        }\n        df['sentiment'] = df['sentiment_label'].map(label_mapping)\n\n        return df\n\n    def get_sentiment_distribution(self, df):\n        '''\n        Get sentiment distribution statistics\n        '''\n        distribution = df['sentiment'].value_counts(normalize=True)\n        return distribution\n\n# Example usage\nif __name__ == \"__main__\":\n    analyzer = DistilBERTSentimentAnalyzer()\n\n    # Example texts\n    sample_texts = [\n        \"I love this product! It's amazing!\",\n        \"This is the worst experience ever.\",\n        \"The weather is okay today.\",\n        \"Absolutely fantastic service!\"\n    ]\n\n    for text in sample_texts:\n        result = analyzer.analyze_single_text(text)\n        print(f\"Text: {text}\")\n        print(f\"Sentiment: {result['label']} (Score: {result['score']:.3f})\\n\")\n        "
  }
}